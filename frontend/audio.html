<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Generated Audio & PDF Viewer</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.16.105/pdf.min.js"></script> <!-- PDF.js -->
    <style>
        
        body {
            background-image: #ffffff;
            color: #000000;
            font-family: "Courier New", monospace;
            text-align: center;
            
        }
        h1 {
            color: #000000;
            font-family: "Courier New", monospace;
        }
        #pdf-container {
            margin-top: 20px;
        }
        canvas {
            border: 1px solid #ffffff;
            max-width: 100%;
            max-height: 650px;
        }
        
        #audio-container {
            position: relative;
            display: flex;
            justify-content: center;
            align-items: center;
           
        
        }
        audio {
            width: 80%;
            max-width: 100%px;
        }
      
     
        
        /* Settings Button */
        #settings-button {
            background: none;
            border: none;
            color: #ffffff;
            font-size: 20px;
            cursor: pointer;
            

        }
        #settings-menu {
            display: none;
            position: fixed;
            bottom: 80px;
            right: 20px;
            background-color: rgba(167, 167, 167, 0.9);
            padding: 15px;
            width: 250px;
            text-align: left;
            border-radius: 8px;
            box-shadow: 0px 4px 10px rgba(255, 255, 255, 0.2);
            z-index: 2000;
        }

        #settings-menu label {
            color: #ffffff;
            font-size: 14px;
            display: block;
            width:50%
        }
        /* Exit Button for Settings */
        #exit-settings {
            display: inline;
            margin-top: 10px;
            padding: 1px;
            background-color: #ff4444;
            color: #ffffff;
            border: none;
            cursor: pointer;
            width: 50%;
            text-align: center;
            font-size: 20px;
        }
        #exit-settings:hover {
            background-color: #772a2a;
        }
        
        #controls {
            margin-top: 10px;

        }
        button {
            padding: 10px;
            margin: 5px;
            font-size: 10px;
            background-color: #ffffff;
            color: #121212;
            border: none;
            cursor: pointer;
            font-family: "Courier New", monospace;
            transition: background 0.3s;
            
        }
        button:hover {
            background-color: #555555;
        }
            /* Chatbot Button */

        #chat-toggle {
            position: fixed;
            bottom: 25px;
            right: 20px;
            width: 50px;
            height: 50px;
            background: #008CBA;
            color: white;
            border: none;
            border-radius: 50%;
            cursor: pointer;
            font-size: 20px;
            display: flex;
            justify-content: center; 
            align-items: center; 
            z-index: 9999; 
            transition: background 0.5s;
        }
        #chat-toggle:hover {
            background-color: #bbbbbb;
        }
        /* Chatbot Window */
        #chatbot-container {
            position: fixed;
            bottom: 80px;
            right: 1px;
            width: 320px;
            height: 400px;
            background: #1a1a1a;
            border: 1px solid #555;
            border-radius: 8px;
            display: none;
            flex-direction: column;
            box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.5);
            opacity: 1;
            z-index: 1000;
        }
        /* Chat Header */
        #chat-header {
            background: #333;
            padding: 10px;
            font-size: 14px;
            color: white;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        /* Close Chat Button */
        #close-chat {
            background: none;
            border: none;
            color: white;
            font-size: 16px;
            cursor: pointer;
        }
        /* Chat Messages Area */
        #chat-area {
            flex-grow: 1;
            padding: 10px;
            overflow-y: auto;
            font-size: 13px;
            max-height: 250px;
            color: white;
            background-color: #1a1a1a;
            border-bottom: 1px solid #444;
        }
        /* Chat Input */
        #chat-input {
            width: calc(100% - 15px);
            padding: 8px;
            border: none;
            border-top: 1px solid #555;
            background: #222;
            color: white;
            font-size: 13px;
            z-index: 1001;
        }
        #settings-menu, #volume-slider{
            z-index: 500;
        }
        /* Send Button */
        #send-chat {
            width: 59px;
            padding: 8px;
            background: #4d4747;
            border: none;
            cursor: pointer;
            color: white;
            font-size: 13px;
        }
        #send-chat:hover{
            background: #413e3e;
        }
        .user-message, .ai-message {
            margin: 5px 0;
            padding: 5px;
            border-radius: 5px;
        }
        .user-message {
            background: #008CBA;
            text-align: right;
        }
        .ai-message {
            background: #3a3a3a;
            text-align: left;
        }
        #home-button {
            position: fixed;
            top: 20px;
            left: 20px;
            background: #008CBA;
            color: white;
            border: none;
            padding: 10px 20px;
            font-size: 16px;
            border-radius: 5px;
            cursor: pointer;
            z-index: 1000;
            transition: background 0.3s;
        }
        #home-button:hover {
            background: #005f75;
        }
        @media (max-width: 600px) {
            h1 {
                font-size: 18px;
            
            }
            #pdf-container {
            margin-top: 20px;
            ;

            }

            #settings-menu {
                width: 40%;
                right: 10%;
                padding: 10px;
 
            }
            #settings-menu label {
                font-size: 12px;
            }
            button{
                font-size: 12px;
                padding: 8px 12px;
                border:1px solid black;
                border-radius:100px;

            }
            #send-chat{
                font-size:12px;
                padding:8px 12px;
            }
            #home-button{
                font-size: 12px;
                padding: 8px 12px;
            }
            #chatbot-container {
                width: 90%;
                right: 5%;
                height: 350px;
            }
        }
        /* Tablet (Min Width: 601px and Max Width: 1024px) */
        @media (min-width: 601px) and (max-width: 1024px) {
            h1 {
                font-size: 24px;
            }
            #settings-menu {
                width: 25%;
                right: 10%;
                padding: 10px;

            }
            #chatbot-container {
                width: 30%;
                right: 2%;
                height: 380px;
            }
            button {
                font-size: 14px;
                padding: 10px 11px;
                border:1px solid black;
                border-radius:100px;
            }
            #send-chat{
                font-size:14px;
                padding:10px 11px;
            }
            #home-button{
                font-size: 14px;
                padding: 10px 11px;
            }
        }
        /*Laptop & Large Screens (Min Width: 1025px) */
        @media (min-width: 1025px) {
            h1 {
                font-size: 28px;
            }
            #settings-menu {
                width: 14%;
                right: 10%;
                padding: 10px;
            }
            #chatbot-container {
                width: 320px;
                height: 400px;
                right: 20px;
            }
            button {
                font-size: 16px;
                padding: 10px 9px; 
                border:1px solid black;
                border-radius:100px;
                
            }
            #send-chat{
                font-size:16px;
                padding:10px 9px;

            }
            #home-button{
                font-size: 16px;
                padding: 10px 9px;
            }
        }
    </style>
</head>
<body>
<button id="chat-toggle" onclick="toggleChat()">💬</button>
<!-- Chatbot Window -->
<div id="chatbot-container">
    <div id="chat-header">
        <span>AI Professor</span>
        <button id="close-chat" onclick="toggleChat()">✖</button>
    </div>
    <div id="chat-area"></div>
    <input type="text" id="chat-input" placeholder="Ask a question..." onkeypress="handleKeyPress(event)">
    <button id="send-chat" onclick="sendMessage()">Send</button>
</div>
<script>
    function toggleChat() {
        let chatbox = document.getElementById("chatbot-container");
        chatbox.style.display = (chatbox.style.display === "none" || chatbox.style.display === "") ? "flex" : "none";
    }
    function handleKeyPress(event) {
        if (event.key === "Enter") sendMessage();
    }
    async function sendMessage() {
        let input = document.getElementById("chat-input");
        let message = input.value.trim();
        if (!message) return;
        let chatArea = document.getElementById("chat-area");
        chatArea.innerHTML += `<div class="user-message">${message}</div>`;
        input.value = "";

        try {
            let response = await fetch("from fastapi import FastAPI, File, UploadFile, Form,HTTPException
from fastapi.responses import JSONResponse
import fitz  # PyMuPDF for PDF text extraction
from pptx import Presentation
import os
import requests
from google.cloud import texttospeech
from dotenv import load_dotenv
import shutil
import uuid
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import random
app = FastAPI()
import json

UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)
app.mount("/uploads", StaticFiles(directory=UPLOAD_DIR), name="uploads")

EXTRACTED_TEXT = ""

@app.get("/")
def root():
    return {"message": "FastAPI is running!"}

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow frontend access
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

client = texttospeech.TextToSpeechClient()
load_dotenv()

def extract_text_from_pdf(file_path):
    global EXTRACTED_TEXT
    doc = fitz.open(file_path)
    text = "\n".join(page.get_text() for page in doc)
    EXTRACTED_TEXT = text.strip() if text.strip() else "Error: No readable text found in PDF."
    return EXTRACTED_TEXT

def extract_text_from_pptx(file_path):
    global EXTRACTED_TEXT
    prs = Presentation(file_path)
    text = "\n".join(shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, "text"))
    EXTRACTED_TEXT = text.strip() if text.strip() else "Error: No readable text found in PPTX."
    return EXTRACTED_TEXT

def generate_lecture_transcript_gemini(extracted_text):
    if not extracted_text or "Error" in extracted_text:
        return "Error: No valid text found in the document."

    gemini_api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={os.getenv('GEMINI_API_KEY')}"

    payload = {
        "contents": [{
            "parts": [{
                "text": f"""You are an AI lecture assistant delivering a **structured, engaging, and interactive lecture** based on the provided slides. Your goal is to generate a **natural, student-friendly AI narration** that feels exactly like a real professor explaining the topic in a live classroom.
                and stricty always make it below 4800 bytes   

    <speak>  

    <break time="1000ms"/>  

    ### **Introduction**  

    Alright class, let's begin. <break time="700ms"/>  

    Today, we are going to explore something really interesting. <break time="500ms"/>  

    Have you ever wondered... <break time="1000ms"/> how artificial intelligence actually learns? <break time="700ms"/>  

    Let's break it down step by step.  

    ---

    ### **Slide-Based Explanation**  

    <break time="1200ms"/>  
    **Now, let's move to Slide 1.** <break time="700ms"/>  

    This slide introduces the concept of <prosody rate="85%"> Machine Learning. </prosody>  
    At its core, machine learning is about making computers learn from data.  

    Think of it like this... <break time="700ms"/>  
    When you play a video game, the more you play, the better you get. <break time="500ms"/>  

    Computers do the same thing – they improve over time, based on data.  

    ---

    <break time="1500ms"/>  
    ### **Mathematical Symbols Explained Naturally**  

    Alright, let's go to **Slide 2**, where we see an important equation:  

    <break time="1000ms"/>  
    Newton's Second Law of Motion.  

    On the slide, you see this equation:  

    <break time="700ms"/>  
    F equals m times a.  

    Notice how I said that? <break time="500ms"/>  

    Instead of saying "asterisk" or "star", I simply said **"times."**  

    So remember, when we see:  

    <break time="700ms"/>  
    **F = m * a**  

    We say:  

    <break time="700ms"/>  
    **"Force is equal to mass multiplied by acceleration."**  

    That makes it sound **natural** and **easy to understand.**  

    ---

    <break time="1500ms"/>  
    ### **Pseudocode Explained Like a Professor**  

    Now, let's go to **Slide 3**. <break time="700ms"/>  

    Here, we see a basic programming logic.  

    On the slide, you see something like this:  

    <break time="700ms"/>  
    "If X is greater than 10, print 'High'."  

    We don’t say **"If open parenthesis X greater than 10 close parenthesis..."** <break time="700ms"/>  

    That sounds **robotic and unnatural.**  

    Instead, a professor would say:  

    <break time="700ms"/>  
    "If X is greater than 10, then the system prints 'High' on the screen."  

    Sounds better, right?  

    ---

    <break time="1500ms"/>  
    ### **Real-World Example to Engage Students**  

    Alright, let's think of an example.  

    <break time="700ms"/>  

    You know how Netflix recommends shows based on what you watch?  

    That's machine learning! <break time="700ms"/>  

    It learns your preferences over time, just like how we get better at playing a game.  

    ---

    <break time="1200ms"/>  
    ### **Encouraging Student Engagement**  

    Now, if any of this isn’t clear, feel free to **ask the chatbot** for another explanation.  

    <break time="700ms"/>  

    Or, if you want a **real-world example**, just type your question in the chatbot!  

    ---

    <break time="2000ms"/>  
    ### **Conclusion**  

    Okay, before we wrap up, let's summarize:  

    <break time="700ms"/>  
    - Machine learning helps computers learn from data.  
    - **Equations should be narrated naturally** (F = m * a → "Force is mass times acceleration").  
    - **Pseudocode should be explained in a spoken way** (Not just reading code literally).  

    ---

    Alright, that's it for today.  

    <break time="1000ms"/>  

    See you in the next lecture!  

    </speak>  

    Now, generate an **engaging, student-friendly lecture transcript** based on the provided slides, while following this **natural narration format**.  

    \n\n{extracted_text}"""
            }]
        }]
    }



    headers = {"Content-Type": "application/json"}

    try:
        print("Using GEMINI_API_KEY:", os.getenv("GEMINI_API_KEY"))  # Log the API key being used

        response = requests.post(gemini_api_url, json=payload, headers=headers, timeout=15)
        print(f"Gemini API Response Status: {response.status_code}")  # Log status code
        print(f"Gemini API Response Body: {response.text}")  # Log full response body

        response_data = response.json()

        if response.status_code == 200 and "candidates" in response_data:
            return response_data["candidates"][0]["content"]["parts"][0]["text"].strip()
        else:
            error_message = response_data.get("error", {}).get("message", "Unknown error")
            print(f"Gemini API Error Message: {error_message}")
            return f"Error generating transcript: {error_message}"

    except requests.exceptions.RequestException as e:
        print(f"Gemini API Request Exception: {e}")
        return "Error: Failed to connect to Gemini API."

    except Exception as e:
        print(f"Unexpected Error in generate_lecture_transcript_gemini: {e}")
        return f"Error: {str(e)}"

def text_to_speech(transcript):
    audio_filename = f"lecture_{uuid.uuid4().hex}.mp3"
    output_audio_path = os.path.join(UPLOAD_DIR, audio_filename)

    synthesis_input = texttospeech.SynthesisInput(text=transcript)

    voice = texttospeech.VoiceSelectionParams(
        language_code="en-US",
        name="en-US-Studio-O",
    )

    audio_config = texttospeech.AudioConfig(
        audio_encoding=texttospeech.AudioEncoding.MP3,
        speaking_rate=1.0
    )

    try:
        print(" Generating AI Audio...")
        response = client.synthesize_speech(
            input=synthesis_input, voice=voice, audio_config=audio_config
        )

        print(" TTS Response received.")

        with open(output_audio_path, "wb") as out:
            out.write(response.audio_content)

        return audio_filename  

    except Exception as e:
        print(f" ERROR generating TTS audio: {e}")
        return None

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    global EXTRACTED_TEXT
    try:
        file_ext = file.filename.split('.')[-1].lower()
        file_path = os.path.join(UPLOAD_DIR, file.filename)

        # Save the uploaded file
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        if not os.path.exists(file_path):
            print(f"ERROR: File {file_path} was not saved properly!")
            return JSONResponse(content={"message": "File save failed."}, status_code=500)

        print(f"File successfully saved at: {file_path}")

        # Extract text based on file type
        if file_ext == 'pdf':
            extracted_text = extract_text_from_pdf(file_path)
        elif file_ext == 'pptx':
            extracted_text = extract_text_from_pptx(file_path)
        else:
            print(f"ERROR: Unsupported file type: {file_ext}")
            return JSONResponse(content={"message": "Unsupported file type."}, status_code=400)

        print(f"Extracted text: {extracted_text[:500]}...")  # Log first 500 characters of extracted text

        # Generate AI transcript
        ai_transcript = generate_lecture_transcript_gemini(extracted_text)
        if "Error" in ai_transcript:
            print(f"ERROR: Transcript generation failed: {ai_transcript}")
            if "No readable text" in ai_transcript:
                return JSONResponse(content={"message": "The uploaded file doesn't contain readable text."}, status_code=400)
            return JSONResponse(content={"message": "Transcript generation failed due to an AI error."}, status_code=400)

        # Generate audio from transcript
        audio_filename = text_to_speech(ai_transcript)
        if not audio_filename:
            print("ERROR: Audio generation failed.")
            return JSONResponse(content={"message": "Audio generation failed."}, status_code=500)

        file_url = f"https://slides-to-ai-powered-lecture-3.onrender.com/uploads/{file.filename}"
        audio_url = f"https://slides-to-ai-powered-lecture-3.onrender.com/{audio_filename}"

        print(f"Returning JSON Response:\nFile URL: {file_url}\nAudio URL: {audio_url}")  # Debugging log

        return JSONResponse(content={
            "file_url": file_url,
            "audio_url": audio_url,
        }, status_code=200)

    except Exception as e:
        print(f"FastAPI Error: {e}")  # Log the exception
        return JSONResponse(content={"error": str(e)}, status_code=500)

@app.post("/chat")
async def chat(query: str = Form(...)):
    global EXTRACTED_TEXT

    if not EXTRACTED_TEXT:
        return JSONResponse(content={
            "response": "I don't have access to the course material. Please upload a PDF or PPTX first.",
            "audio_url": None
        })

    if not query.strip():
        return JSONResponse(content={"response": "Please ask a valid question.", "audio_url": None})

    gemini_api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={os.getenv('GEMINI_API_KEY')}"

    payload = {
        "contents": [{
            "parts": [{
                "text": f"""
                You are an AI Proferssor with access to the following document:

                ---
                {EXTRACTED_TEXT}
                ---

                Your primary task is to answer the question from anywhere when the user askes
                question about the document. The question should be stricly from the document 
                provided

                **Question:** {query}
                """
            }]
        }]
    }

    headers = {"Content-Type": "application/json"}

    try:
        response = requests.post(gemini_api_url, json=payload, headers=headers, timeout=15)
        response_data = response.json()

        if response.status_code == 200 and "candidates" in response_data:
            response_text = response_data["candidates"][0]["content"]["parts"][0]["text"].strip()
        else:
            response_text = "I couldn't find a relevant answer in the course material."

    except Exception as e:
        print(f" Error calling Gemini API: {e}")
        response_text = "Error generating AI response."

    audio_filename = text_to_speech(response_text) if response_text else None
    audio_url = f"https://slides-to-ai-powered-lecture-3.onrender.com/uploads/{audio_filename}" if audio_filename else None

    return JSONResponse(content={"response": response_text, "audio_url": audio_url})

@app.post("/generate_quiz")
async def generate_quiz():
    global EXTRACTED_TEXT

    if not EXTRACTED_TEXT:
        raise HTTPException(status_code=400, detail="No transcript found. Upload slides first.")

    gemini_api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={os.getenv('GEMINI_API_KEY')}"

    payload = {
        "contents": [{
            "parts": [{
                "text": f"Generate a 10-question quiz from the following transcript. Ensure a mix of multiple-choice , true/false questions or anything that can fit under the criteria of multiple choice question. Format it as a **valid JSON array**, where each object contains 'question', 'options', and 'answer'. Do **not** return markdown formatting (```json) or any code blocks:\n\n{EXTRACTED_TEXT}"
            }]
        }]
    }

    headers = {"Content-Type": "application/json"}

    try:
        response = requests.post(gemini_api_url, json=payload, headers=headers, timeout=15)
        response_data = response.json()

        if response.status_code == 200 and "candidates" in response_data:
            raw_quiz_text = response_data["candidates"][0]["content"]["parts"][0]["text"].strip()

            try:
                quiz_data = json.loads(raw_quiz_text)
            except json.JSONDecodeError:
                return JSONResponse(content={"message": "Invalid AI response format"}, status_code=500)

            return JSONResponse(content={"quiz": quiz_data})
        else:
            return JSONResponse(content={"message": "Quiz generation failed."}, status_code=400)

    except Exception as e:
        print(f" Error generating quiz: {e}")
        return JSONResponse(content={"error": "Quiz generation error."}, status_code=500)/chat", {
                method: "POST",
                headers: { "Content-Type": "application/x-www-form-urlencoded" },
                body: `query=${encodeURIComponent(message)}`
            });
            let data = await response.json();
            let botResponse = data.response || "Error processing request.";
            chatArea.innerHTML += `<div class="ai-message">${botResponse}</div>`;
            if (data.audio_url) {
                chatArea.innerHTML += `<audio controls><source src="${data.audio_url}" type="audio/mp3"></audio>`;
            }
            chatArea.scrollTop = chatArea.scrollHeight;
        } catch (error) {
            chatArea.innerHTML += `<div class="ai-message">Error: Could not connect to AI server.</div>`;
        }
    }
</script>
    <!-- Title -->
    <h1>AI POWERED LECTURE</h1>
    <!-- PDF Viewer -->
    
    <div id="pdf-container">
        <canvas id="pdfCanvas"></canvas>
    </div>
    <!-- Audio Player with Volume Slider & Settings Button -->
     
    <div id="audio-container">
        <audio id="audioPlayer" controls>
            <source src="" type="audio/mp3">
        </audio>
        

    <!-- Settings Button -->
    <button id="settings-button" onclick="toggleSettings()">⚙️</button>
    </div> 
    <!-- Settings Dropdown -->
    <div id="settings-menu">
    <label for="speed-slider">Playback Speed: <span id="speed-value">1.0x</span></label>
    <input type="range" id="speed-slider" min="0.5" max="2" step="0.1" value="1.0" oninput="changeSpeed()">
    
    <!-- Download Audio Button -->
    <button id="download-audio" onclick="downloadAudio()">Download Audio</button>
    
    <!-- Exit Button Inside Settings Menu -->
    <button id="exit-settings" onclick="toggleSettings()">⏎</button>

    <style>
        .button {
            margin-top: 20px;
            background-color: #7a7e83;
            color: white;
            border: none;
            padding: 12px 20px;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            transition: background 0.3s;
        }

        .button:hover {
            background-color: #000000;
        }
    </style>

    </div>

    <script>
        function downloadAudio() {
            let audioElement = document.getElementById("audioPlayer");
            let audioSource = audioElement.querySelector("source").src;

            if (!audioSource) {
                alert("No audio file available for download.");
                return;
            }

            let downloadLink = document.createElement("a");
            downloadLink.href = audioSource;
            downloadLink.download = "Generated_Audio.mp3";
            document.body.appendChild(downloadLink);
            downloadLink.click();
            document.body.removeChild(downloadLink);
        }
    </script>
    

    

  

    <!-- Slide Controls -->
<!-- Navigation buttons moved below the play button -->
    <div id="navigation-container">
        <button id="prev-button" onclick="prevPage()">Previous</button>
        <span>Page: <span id="pageNum">1</span> / <span id="pageCount">?</span></span>
        <button id="next-button" onclick="nextPage()">Next</button>

    </div>
    <div>
        <button class="button" onclick="window.location.href='quiz.html'">GENERATE QUIZ</button>
        
    </div>   


    <script>
        let pdfDoc = null,
            pageNum = 1,
            pageCount = 0,
            params = new URLSearchParams(window.location.search),
            fileurl = params.get("file_url") ? decodeURIComponent(params.get("file_url")) : null,
            audioUrl = params.get("audio_url") ? decodeURIComponent(params.get("audio_url")) : null,
            canvas = document.getElementById("pdfCanvas"),
            ctx = canvas.getContext("2d");

        console.log("Full Query String:", window.location.search);
        console.log("Extracted PDF URL:", fileurl); 
        console.log("Extracted Audio URL:", audioUrl);

        function toggleSettings() {
            let menu = document.getElementById("settings-menu");
            menu.style.display = (menu.style.display === "none" || menu.style.display === "") ? "block" : "none";
        }

        function changeSpeed() {
            let audioPlayer = document.getElementById("audioPlayer");
            let speedSlider = document.getElementById("speed-slider");
            let speedValue = document.getElementById("speed-value");

            let speed = parseFloat(speedSlider.value);
            audioPlayer.playbackRate = speed;
            speedValue.innerText = speed.toFixed(1) + "x";
        }

        function changeVolume() {
            let audioPlayer = document.getElementById("audioPlayer");
            let volumeSlider = document.getElementById("volume-slider");

            audioPlayer.volume = parseFloat(volumeSlider.value);
        }

        function loadPdf(url) {
            fetch(url).then(response => response.blob()).then(blob => {
                let pdfUrl = URL.createObjectURL(blob);
                pdfjsLib.getDocument(pdfUrl).promise.then(pdf => {
                    pdfDoc = pdf;
                    pageCount = pdf.numPages;
                    document.getElementById("pageCount").innerText = pageCount;
                    renderPage(pageNum);
                });
            }).catch(error => {
                console.error("Error loading PDF:", error);
            });
        }

        function renderPage(num) {
            pdfDoc.getPage(num).then(page => {
                let viewport = page.getViewport({ scale: 1.5 });
                canvas.width = viewport.width;
                canvas.height = viewport.height;

                let renderContext = {
                    canvasContext: ctx,
                    viewport: viewport
                };

                page.render(renderContext).promise.then(() => {
                    document.getElementById("pageNum").innerText = num;
                });
            });
        }

        function nextPage() {
            if (pageNum < pageCount) {
                pageNum++;
                renderPage(pageNum);
            }
        }

        function prevPage() {
            if (pageNum > 1) {
                pageNum--;
                renderPage(pageNum);
            }
        }

        window.onload = function () {
            if (fileurl) loadPdf(fileurl);
            if (audioUrl) {
                document.getElementById("audioPlayer").src = audioUrl;
                document.getElementById("audioPlayer").style.display = "block";
            }
        };
    </script>

<script>
    function askQuestion() {
        let input = document.getElementById("chat-input").value.trim();
        if (!input) return;
        let chatLog = document.getElementById("chat-log");

        // Display user's question
        chatLog.innerHTML += `<p><strong>You:</strong> ${input}</p>`;

        // Send question to FastAPI backend
        fetch("from fastapi import FastAPI, File, UploadFile, Form,HTTPException
from fastapi.responses import JSONResponse
import fitz  # PyMuPDF for PDF text extraction
from pptx import Presentation
import os
import requests
from google.cloud import texttospeech
from dotenv import load_dotenv
import shutil
import uuid
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import random
app = FastAPI()
import json

UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)
app.mount("/uploads", StaticFiles(directory=UPLOAD_DIR), name="uploads")

EXTRACTED_TEXT = ""

@app.get("/")
def root():
    return {"message": "FastAPI is running!"}

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow frontend access
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

client = texttospeech.TextToSpeechClient()
load_dotenv()

def extract_text_from_pdf(file_path):
    global EXTRACTED_TEXT
    doc = fitz.open(file_path)
    text = "\n".join(page.get_text() for page in doc)
    EXTRACTED_TEXT = text.strip() if text.strip() else "Error: No readable text found in PDF."
    return EXTRACTED_TEXT

def extract_text_from_pptx(file_path):
    global EXTRACTED_TEXT
    prs = Presentation(file_path)
    text = "\n".join(shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, "text"))
    EXTRACTED_TEXT = text.strip() if text.strip() else "Error: No readable text found in PPTX."
    return EXTRACTED_TEXT

def generate_lecture_transcript_gemini(extracted_text):
    if not extracted_text or "Error" in extracted_text:
        return "Error: No valid text found in the document."

    gemini_api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={os.getenv('GEMINI_API_KEY')}"

    payload = {
        "contents": [{
            "parts": [{
                "text": f"""You are an AI lecture assistant delivering a **structured, engaging, and interactive lecture** based on the provided slides. Your goal is to generate a **natural, student-friendly AI narration** that feels exactly like a real professor explaining the topic in a live classroom.
                and stricty always make it below 4800 bytes   

    <speak>  

    <break time="1000ms"/>  

    ### **Introduction**  

    Alright class, let's begin. <break time="700ms"/>  

    Today, we are going to explore something really interesting. <break time="500ms"/>  

    Have you ever wondered... <break time="1000ms"/> how artificial intelligence actually learns? <break time="700ms"/>  

    Let's break it down step by step.  

    ---

    ### **Slide-Based Explanation**  

    <break time="1200ms"/>  
    **Now, let's move to Slide 1.** <break time="700ms"/>  

    This slide introduces the concept of <prosody rate="85%"> Machine Learning. </prosody>  
    At its core, machine learning is about making computers learn from data.  

    Think of it like this... <break time="700ms"/>  
    When you play a video game, the more you play, the better you get. <break time="500ms"/>  

    Computers do the same thing – they improve over time, based on data.  

    ---

    <break time="1500ms"/>  
    ### **Mathematical Symbols Explained Naturally**  

    Alright, let's go to **Slide 2**, where we see an important equation:  

    <break time="1000ms"/>  
    Newton's Second Law of Motion.  

    On the slide, you see this equation:  

    <break time="700ms"/>  
    F equals m times a.  

    Notice how I said that? <break time="500ms"/>  

    Instead of saying "asterisk" or "star", I simply said **"times."**  

    So remember, when we see:  

    <break time="700ms"/>  
    **F = m * a**  

    We say:  

    <break time="700ms"/>  
    **"Force is equal to mass multiplied by acceleration."**  

    That makes it sound **natural** and **easy to understand.**  

    ---

    <break time="1500ms"/>  
    ### **Pseudocode Explained Like a Professor**  

    Now, let's go to **Slide 3**. <break time="700ms"/>  

    Here, we see a basic programming logic.  

    On the slide, you see something like this:  

    <break time="700ms"/>  
    "If X is greater than 10, print 'High'."  

    We don’t say **"If open parenthesis X greater than 10 close parenthesis..."** <break time="700ms"/>  

    That sounds **robotic and unnatural.**  

    Instead, a professor would say:  

    <break time="700ms"/>  
    "If X is greater than 10, then the system prints 'High' on the screen."  

    Sounds better, right?  

    ---

    <break time="1500ms"/>  
    ### **Real-World Example to Engage Students**  

    Alright, let's think of an example.  

    <break time="700ms"/>  

    You know how Netflix recommends shows based on what you watch?  

    That's machine learning! <break time="700ms"/>  

    It learns your preferences over time, just like how we get better at playing a game.  

    ---

    <break time="1200ms"/>  
    ### **Encouraging Student Engagement**  

    Now, if any of this isn’t clear, feel free to **ask the chatbot** for another explanation.  

    <break time="700ms"/>  

    Or, if you want a **real-world example**, just type your question in the chatbot!  

    ---

    <break time="2000ms"/>  
    ### **Conclusion**  

    Okay, before we wrap up, let's summarize:  

    <break time="700ms"/>  
    - Machine learning helps computers learn from data.  
    - **Equations should be narrated naturally** (F = m * a → "Force is mass times acceleration").  
    - **Pseudocode should be explained in a spoken way** (Not just reading code literally).  

    ---

    Alright, that's it for today.  

    <break time="1000ms"/>  

    See you in the next lecture!  

    </speak>  

    Now, generate an **engaging, student-friendly lecture transcript** based on the provided slides, while following this **natural narration format**.  

    \n\n{extracted_text}"""
            }]
        }]
    }



    headers = {"Content-Type": "application/json"}

    try:
        print("Using GEMINI_API_KEY:", os.getenv("GEMINI_API_KEY"))  # Log the API key being used

        response = requests.post(gemini_api_url, json=payload, headers=headers, timeout=15)
        print(f"Gemini API Response Status: {response.status_code}")  # Log status code
        print(f"Gemini API Response Body: {response.text}")  # Log full response body

        response_data = response.json()

        if response.status_code == 200 and "candidates" in response_data:
            return response_data["candidates"][0]["content"]["parts"][0]["text"].strip()
        else:
            error_message = response_data.get("error", {}).get("message", "Unknown error")
            print(f"Gemini API Error Message: {error_message}")
            return f"Error generating transcript: {error_message}"

    except requests.exceptions.RequestException as e:
        print(f"Gemini API Request Exception: {e}")
        return "Error: Failed to connect to Gemini API."

    except Exception as e:
        print(f"Unexpected Error in generate_lecture_transcript_gemini: {e}")
        return f"Error: {str(e)}"

def text_to_speech(transcript):
    audio_filename = f"lecture_{uuid.uuid4().hex}.mp3"
    output_audio_path = os.path.join(UPLOAD_DIR, audio_filename)

    synthesis_input = texttospeech.SynthesisInput(text=transcript)

    voice = texttospeech.VoiceSelectionParams(
        language_code="en-US",
        name="en-US-Studio-O",
    )

    audio_config = texttospeech.AudioConfig(
        audio_encoding=texttospeech.AudioEncoding.MP3,
        speaking_rate=1.0
    )

    try:
        print(" Generating AI Audio...")
        response = client.synthesize_speech(
            input=synthesis_input, voice=voice, audio_config=audio_config
        )

        print(" TTS Response received.")

        with open(output_audio_path, "wb") as out:
            out.write(response.audio_content)

        return audio_filename  

    except Exception as e:
        print(f" ERROR generating TTS audio: {e}")
        return None

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    global EXTRACTED_TEXT
    try:
        file_ext = file.filename.split('.')[-1].lower()
        file_path = os.path.join(UPLOAD_DIR, file.filename)

        # Save the uploaded file
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        if not os.path.exists(file_path):
            print(f"ERROR: File {file_path} was not saved properly!")
            return JSONResponse(content={"message": "File save failed."}, status_code=500)

        print(f"File successfully saved at: {file_path}")

        # Extract text based on file type
        if file_ext == 'pdf':
            extracted_text = extract_text_from_pdf(file_path)
        elif file_ext == 'pptx':
            extracted_text = extract_text_from_pptx(file_path)
        else:
            print(f"ERROR: Unsupported file type: {file_ext}")
            return JSONResponse(content={"message": "Unsupported file type."}, status_code=400)

        print(f"Extracted text: {extracted_text[:500]}...")  # Log first 500 characters of extracted text

        # Generate AI transcript
        ai_transcript = generate_lecture_transcript_gemini(extracted_text)
        if "Error" in ai_transcript:
            print(f"ERROR: Transcript generation failed: {ai_transcript}")
            if "No readable text" in ai_transcript:
                return JSONResponse(content={"message": "The uploaded file doesn't contain readable text."}, status_code=400)
            return JSONResponse(content={"message": "Transcript generation failed due to an AI error."}, status_code=400)

        # Generate audio from transcript
        audio_filename = text_to_speech(ai_transcript)
        if not audio_filename:
            print("ERROR: Audio generation failed.")
            return JSONResponse(content={"message": "Audio generation failed."}, status_code=500)

        file_url = f"https://slides-to-ai-powered-lecture-3.onrender.com/uploads/{file.filename}"
        audio_url = f"https://slides-to-ai-powered-lecture-3.onrender.com/{audio_filename}"

        print(f"Returning JSON Response:\nFile URL: {file_url}\nAudio URL: {audio_url}")  # Debugging log

        return JSONResponse(content={
            "file_url": file_url,
            "audio_url": audio_url,
        }, status_code=200)

    except Exception as e:
        print(f"FastAPI Error: {e}")  # Log the exception
        return JSONResponse(content={"error": str(e)}, status_code=500)

@app.post("/chat")
async def chat(query: str = Form(...)):
    global EXTRACTED_TEXT

    if not EXTRACTED_TEXT:
        return JSONResponse(content={
            "response": "I don't have access to the course material. Please upload a PDF or PPTX first.",
            "audio_url": None
        })

    if not query.strip():
        return JSONResponse(content={"response": "Please ask a valid question.", "audio_url": None})

    gemini_api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={os.getenv('GEMINI_API_KEY')}"

    payload = {
        "contents": [{
            "parts": [{
                "text": f"""
                You are an AI Proferssor with access to the following document:

                ---
                {EXTRACTED_TEXT}
                ---

                Your primary task is to answer the question from anywhere when the user askes
                question about the document. The question should be stricly from the document 
                provided

                **Question:** {query}
                """
            }]
        }]
    }

    headers = {"Content-Type": "application/json"}

    try:
        response = requests.post(gemini_api_url, json=payload, headers=headers, timeout=15)
        response_data = response.json()

        if response.status_code == 200 and "candidates" in response_data:
            response_text = response_data["candidates"][0]["content"]["parts"][0]["text"].strip()
        else:
            response_text = "I couldn't find a relevant answer in the course material."

    except Exception as e:
        print(f" Error calling Gemini API: {e}")
        response_text = "Error generating AI response."

    audio_filename = text_to_speech(response_text) if response_text else None
    audio_url = f"https://slides-to-ai-powered-lecture-3.onrender.com/uploads/{audio_filename}" if audio_filename else None

    return JSONResponse(content={"response": response_text, "audio_url": audio_url})

@app.post("/generate_quiz")
async def generate_quiz():
    global EXTRACTED_TEXT

    if not EXTRACTED_TEXT:
        raise HTTPException(status_code=400, detail="No transcript found. Upload slides first.")

    gemini_api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={os.getenv('GEMINI_API_KEY')}"

    payload = {
        "contents": [{
            "parts": [{
                "text": f"Generate a 10-question quiz from the following transcript. Ensure a mix of multiple-choice , true/false questions or anything that can fit under the criteria of multiple choice question. Format it as a **valid JSON array**, where each object contains 'question', 'options', and 'answer'. Do **not** return markdown formatting (```json) or any code blocks:\n\n{EXTRACTED_TEXT}"
            }]
        }]
    }

    headers = {"Content-Type": "application/json"}

    try:
        response = requests.post(gemini_api_url, json=payload, headers=headers, timeout=15)
        response_data = response.json()

        if response.status_code == 200 and "candidates" in response_data:
            raw_quiz_text = response_data["candidates"][0]["content"]["parts"][0]["text"].strip()

            try:
                quiz_data = json.loads(raw_quiz_text)
            except json.JSONDecodeError:
                return JSONResponse(content={"message": "Invalid AI response format"}, status_code=500)

            return JSONResponse(content={"quiz": quiz_data})
        else:
            return JSONResponse(content={"message": "Quiz generation failed."}, status_code=400)

    except Exception as e:
        print(f" Error generating quiz: {e}")
        return JSONResponse(content={"error": "Quiz generation error."}, status_code=500)/chat", {
            method: "POST",
            headers: { "Content-Type": "application/x-www-form-urlencoded" },
            body: new URLSearchParams({ query: input })
        })
        .then(response => response.json())
        .then(data => {
            if (data && data.response) {
                let botResponse = data.response;
                let audioUrl = data.audio_url || null;

                chatLog.innerHTML += `<p><strong>AI:</strong> ${botResponse}</p>`;

                if (audioUrl) {
                    let audioPlayer = new Audio(audioUrl);
                    audioPlayer.play();
                }
            } else {
                chatLog.innerHTML += `<p><strong>AI:</strong> Error processing request.</p>`;
            }
        })
        .catch(error => {
            console.error("Chatbot request failed:", error);
            chatLog.innerHTML += `<p><strong>AI:</strong> Error processing request.</p>`;
        });

        document.getElementById("chat-input").value = "";
    }
    
</script>


</body>
</html>
